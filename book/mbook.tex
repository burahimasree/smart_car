\documentclass[12pt, a4paper, twoside, openright]{book}

% ============================================================================
% PACKAGES
% ============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage{graphicx}
\usepackage{bookmark}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage{float}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{emptypage}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{ragged2e}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{array}
\usepackage{rotating}
\usepackage{pdflscape}

% ============================================================================
% PAGE GEOMETRY (Indian Engineering Standards)
% ============================================================================
\geometry{
    a4paper,
    inner=3.5cm,
    outer=1.5cm,
    top=2.5cm,
    bottom=2.5cm
}

% Line spacing
\onehalfspacing

% ============================================================================
% COLORS
% ============================================================================
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.94}
\definecolor{chaptergrey}{rgb}{0.3,0.3,0.3}
\definecolor{linkblue}{rgb}{0.0,0.4,0.7}

% ============================================================================
% CODE LISTING STYLE
% ============================================================================
\lstdefinestyle{pythonstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen}\itshape,
    keywordstyle=\color{blue}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=8pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=4,
    frame=single,
    rulecolor=\color{black!30},
    xleftmargin=15pt,
    framexleftmargin=15pt,
}

\lstdefinestyle{cppstyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen}\itshape,
    keywordstyle=\color{blue}\bfseries,
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    captionpos=b,
    numbers=left,
    numbersep=8pt,
    frame=single,
    language=C++,
}

\lstset{style=pythonstyle, language=Python}

% ============================================================================
% CHAPTER FORMATTING
% ============================================================================
\titleformat{\chapter}[display]
  {\normalfont\bfseries\Huge}
  {\filleft\fontsize{60}{70}\selectfont\color{chaptergrey}\thechapter}
  {1ex}
  {\titlerule[2pt]\vspace{1ex}\filleft}
  [\vspace{1ex}\titlerule]

% ============================================================================
% HEADER/FOOTER
% ============================================================================
\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE]{\thepage\hspace{1em}\textit{\leftmark}}
\fancyhead[RO]{\textit{\rightmark}\hspace{1em}\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% ============================================================================
% HYPERREF SETUP
% ============================================================================
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=linkblue,
    urlcolor=linkblue,
    pdftitle={The Smart Car Project: Complete Technical Reference},
    pdfauthor={Bura Himasree, Kamal Bura},
}

% ============================================================================
% DOCUMENT BEGIN
% ============================================================================
\begin{document}

% ============================================================================
% TITLE PAGE
% ============================================================================
\begin{titlepage}
\centering
\vspace*{2cm}

{\Huge\bfseries The Smart Car Project}\\[0.5cm]
{\Large An Industrial-Grade Autonomous Robotics Platform}\\[1cm]

\rule{\linewidth}{0.5mm}\\[0.5cm]

{\large\textit{Complete Technical Reference\\From Electrons to Execution}}\\[2cm]

\vfill

\begin{flushleft}
\textbf{Project Lead \& Architect}\\
\textsc{Bura Himasree}\\[0.5cm]

\textbf{Hardware \& Systems Engineer}\\
\textsc{Kamal Bura}\\[0.5cm]
\end{flushleft}

\vfill

{\large Version 1.0}\\
{\large January 2026}\\[1cm]

{\small Total Lines of Code Analyzed: 5,200+}\\
{\small Total Files in Repository: 157}

\end{titlepage}

% ============================================================================
% CERTIFICATE PAGE
% ============================================================================
\thispagestyle{empty}
\chapter*{Certificate}
\addcontentsline{toc}{chapter}{Certificate}

\begin{center}
    {\large \textbf{DEPARTMENT OF COMPUTER SCIENCE \& ENGINEERING}}\\
    \vspace{1cm}
    {\Large \textbf{CERTIFICATE}}
\end{center}

\vspace{1cm}

This is to certify that the project report entitled \textbf{``THE SMART CAR PROJECT: AN INDUSTRIAL-GRADE AUTONOMOUS ROBOTICS PLATFORM''} is a bonafide record of the work carried out by \textbf{Bura Himasree} under supervision and guidance, in partial fulfillment of the requirements for the degree of Bachelor of Technology in Computer Science \& Engineering.

\vspace{2cm}

\begin{minipage}{0.45\textwidth}
\textbf{Project Supervisor}\\
\rule{5cm}{0.4pt}\\
Name \& Designation
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\raggedleft
\textbf{Head of Department}\\
\rule{5cm}{0.4pt}\\
Name \& Designation
\end{minipage}

\vspace{2cm}

\noindent\textbf{External Examiner}: \rule{8cm}{0.4pt}

\vspace{1cm}
\noindent Date: \rule{4cm}{0.4pt} \hfill Place: \rule{4cm}{0.4pt}

\clearpage

% ============================================================================
% DECLARATION
% ============================================================================
\thispagestyle{empty}
\chapter*{Declaration}
\addcontentsline{toc}{chapter}{Declaration}

We, \textbf{Bura Himasree} and \textbf{Kamal Bura}, hereby declare that the work presented in this project report entitled \textbf{``THE SMART CAR PROJECT''} is an original record of work carried out by us.

We further declare that the results of this project work have not been submitted to any other university or institute for the award of any degree or diploma.

\vspace{3cm}

\begin{flushright}
\textbf{Bura Himasree}\\[0.5cm]
\textbf{Kamal Bura}
\end{flushright}

\vspace{2cm}

\noindent Date: \today

\clearpage

% ============================================================================
% ACKNOWLEDGEMENT
% ============================================================================
\thispagestyle{empty}
\chapter*{Acknowledgement}
\addcontentsline{toc}{chapter}{Acknowledgement}

The successful completion of this project is the outcome of the coordination, guidance, and assistance of many individuals.

We express our profound gratitude to our project guide for their constant encouragement, invaluable guidance, and scholarly advice throughout the duration of this project.

We extend our thanks to the technical staff of the robotics lab and our fellow students for their assistance. Special thanks to the student contributors who participated in various testing phases.

Finally, we thank our families for their unwavering support during the long hours spent in the hardware lab.

\vspace{2cm}

\begin{flushright}
\textbf{Bura Himasree}\\
\textbf{Kamal Bura}
\end{flushright}

\clearpage

% ============================================================================
% ABSTRACT
% ============================================================================
\chapter*{Abstract}
\addcontentsline{toc}{chapter}{Abstract}

This document presents the complete technical architecture of an autonomous voice-controlled robotic platform built on commodity hardware. Unlike academic prototypes, this system was engineered for real-world deployment, emphasizing reliability, modularity, and measurable performance.

\section*{The Challenge}
Building a robot that can simultaneously listen, see, think, and act requires solving difficult resource contention problems. Most hobby-grade implementations use a single-threaded loop that freezes when any subsystem demands computation.

\section*{Our Approach}
We implemented a \textbf{Service-Oriented Architecture (SOA)} on a Raspberry Pi 4, using ZeroMQ for inter-process communication. Each sensory modality runs as an independent service, publishing events to a central orchestrator.

\textbf{Key Innovations:}
\begin{itemize}[noitemsep]
    \item \textbf{Phase-Driven State Machine}: A 17-transition FSM preventing illegal states
    \item \textbf{Dual-Brain Architecture}: High-level cognition on Pi, low-level reflexes on ESP32
    \item \textbf{Hardware Interlocks}: ESP32 can override Pi commands for safety
    \item \textbf{Latest-Frame Grabber}: Threading pattern eliminating camera buffer lag
\end{itemize}

\section*{Results}
The system achieves a \textbf{2.8-second end-to-end latency} from voice command to wheel motion, operating reliably at 2.0 GHz with thermal stability below 60°C.

\vspace{1cm}
\textbf{Keywords}: Embedded AI, ZeroMQ, Raspberry Pi, ESP32, Voice Assistant, Computer Vision, YOLO, Porcupine, Faster-Whisper

\clearpage

% ============================================================================
% TABLE OF CONTENTS
% ============================================================================
\frontmatter
\tableofcontents
\listoffigures
\listoftables

% ============================================================================
% MAIN MATTER
% ============================================================================
\mainmatter

% ============================================================================
% PART I: SYSTEM OVERVIEW
% ============================================================================
\part{System Overview}

\chapter{Introduction and Motivation}

\section{The Vision}

The Smart Car project is an \textbf{industrial-grade autonomous robotics platform} built on commodity hardware. Unlike academic prototypes that demonstrate concepts but lack real-world reliability, this system was engineered to be:

\begin{enumerate}
    \item \textbf{Production-Ready}: Every component has error handling, timeouts, and graceful degradation
    \item \textbf{Maintainable}: Clear separation of concerns via Service-Oriented Architecture
    \item \textbf{Observable}: Comprehensive logging, LED status indicators, and display feedback
    \item \textbf{Safe}: Hardware-level collision avoidance independent of software state
\end{enumerate}

\section{Core Capabilities}

The robot can:
\begin{itemize}
    \item \textbf{Listen} for a custom wake word (``Hey Veera'') using Porcupine
    \item \textbf{Transcribe} speech using Faster-Whisper with int8 quantization
    \item \textbf{Think} using Google Gemini API with conversation memory
    \item \textbf{Speak} using Piper TTS
    \item \textbf{See} using YOLOv11 object detection
    \item \textbf{Move} via L298N motor driver with ESP32 collision avoidance
\end{itemize}

\section{Key Technical Decisions}

\begin{table}[H]
\centering
\caption{Design Choices and Rationale}
\begin{tabular}{|l|l|p{6cm}|}
\hline
\textbf{Choice} & \textbf{Alternative} & \textbf{Rationale} \\ \hline
ZeroMQ IPC & ROS & 15ms startup vs 30s; single pip install \\ \hline
ESP32 Brainstem & Pure Pi & Deterministic 50ms loop; hardware interlocks \\ \hline
Faster-Whisper & Cloud STT & Privacy; offline; ~800ms latency \\ \hline
Phase-Driven FSM & Event listeners & Illegal states impossible; explicit transitions \\ \hline
\end{tabular}
\end{table}

\chapter{Repository Structure}

\section{Top-Level Layout}

The repository contains \textbf{157 files} organized into functional directories:

\begin{verbatim}
smart_car/
+-- book/               # LaTeX documentation
+-- config/             # YAML configuration (5 files)
+-- docs/               # Markdown docs (24 files)
+-- models/             # AI model weights
+-- scripts/            # Utility scripts (43 files)
+-- src/                # Python source (54 files)
+-- systemd/            # Service units (8 files)
+-- tools/              # Diagnostic tools (22 files)
+-- .venvs/             # Virtual environments
+-- logs/               # Runtime logs
\end{verbatim}

\section{Source Code Organization}

The \texttt{src/} directory contains 11 subdirectories:

\begin{table}[H]
\centering
\caption{Source Code Modules}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Directory} & \textbf{Files} & \textbf{Purpose} \\ \hline
audio/ & 5 & Voice pipeline (Wakeword + STT) \\ \hline
core/ & 5 & Orchestrator, IPC, Config \\ \hline
llm/ & 4 & Gemini LLM integration \\ \hline
piled/ & 2 & LED ring control \\ \hline
stt/ & 4 & Speech-to-text engines \\ \hline
tts/ & 3 & Text-to-speech engines \\ \hline
uart/ & 4 & ESP32 motor bridge \\ \hline
ui/ & 4 & TFT display rendering \\ \hline
vision/ & 5 & YOLO object detection \\ \hline
tests/ & 11 & Unit tests \\ \hline
\end{tabular}
\end{table}

\chapter{System Architecture}

\section{Dual-Brain Architecture}

The system uses a \textbf{Dual-Brain Architecture} separating high-level cognition from low-level reflexes:

\begin{figure}[H]
\centering
\begin{verbatim}
+-------------------+      UART      +------------------+
| Raspberry Pi 4B   |<-------------->|  ESP32           |
| (The Cortex)      |   115200 baud  |  (The Brainstem) |
|                   |                |                  |
| - Orchestrator    |                | - Motor Control  |
| - Voice Pipeline  |                | - Sensor Polling |
| - Vision Pipeline |                | - Collision Stop |
| - Gemini LLM      |                +------------------+
+-------------------+                        |
       |                                     |
  ZMQ Bus                               L298N / Motors
  (TCP 6010/6011)                       HC-SR04 x3
       |                                MQ3 Gas Sensor
+------+--------+
|               |
v               v
[Display]    [LED Ring]
\end{verbatim}
\caption{System Topology: Dual-Brain Architecture}
\end{figure}

\textbf{Why Two Processors?}

Linux is not a real-time operating system. Python garbage collection, network interrupts, or disk I/O can delay code execution by hundreds of milliseconds. For safety-critical tasks like collision avoidance, this is unacceptable.

The ESP32 provides:
\begin{itemize}
    \item \textbf{Deterministic 50ms control loop} (20Hz updates)
    \item \textbf{Hardware PWM generation} without software jitter
    \item \textbf{Independent collision override} even if Pi software crashes
\end{itemize}

\section{Message Flow}

All inter-process communication uses ZeroMQ PUB/SUB on two ports:

\begin{table}[H]
\centering
\caption{IPC Port Allocation}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Port} & \textbf{Name} & \textbf{Direction} & \textbf{Purpose} \\ \hline
6010 & Upstream & Sensor → Orchestrator & Events from subsystems \\ \hline
6011 & Downstream & Orchestrator → Actuators & Commands to subsystems \\ \hline
\end{tabular}
\end{table}

\section{Phase-Driven State Machine}

The Orchestrator implements a \textbf{17-transition Finite State Machine}:

\begin{figure}[H]
\centering
\begin{verbatim}
                   [wakeword]
           +----------- IDLE <-----------+
           |                             |
           v                             |
      LISTENING ----[stt_valid]----> THINKING
           |                             |
     [stt_timeout]              [llm_with_speech]
           |                             |
           +-----------------------------v
                                     SPEAKING
                                         |
                                    [tts_done]
                                         |
                                         v
                                       IDLE
\end{verbatim}
\caption{Orchestrator State Machine (Simplified)}
\end{figure}

% ============================================================================
% PART II: HARDWARE ARCHITECTURE
% ============================================================================
\part{Hardware Architecture}

\chapter{Hardware Components}

\section{Bill of Materials}

\begin{table}[H]
\centering
\caption{Hardware Bill of Materials}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Component} & \textbf{Model} & \textbf{Purpose} & \textbf{Interface} \\ \hline
SBC & Raspberry Pi 4B 8GB & Main compute & - \\ \hline
Microcontroller & ESP32 DevKit & Motor control & UART \\ \hline
Motor Driver & L298N H-Bridge & DC motors & GPIO \\ \hline
Ultrasonic (x3) & HC-SR04 & Distance sensing & GPIO \\ \hline
Gas Sensor & MQ-3 & Air quality & ADC \\ \hline
Camera & 5MP CSI Module & Vision & CSI \\ \hline
Microphone & USB Sound Card & Audio input & USB \\ \hline
Display & Waveshare 3.5" TFT & UI & SPI \\ \hline
LEDs & NeoPixel Ring (8) & Status & GPIO12 \\ \hline
\end{tabular}
\end{table}

\section{Raspberry Pi Configuration}

\subsection{Overclocking}

The stock Raspberry Pi 4 operates at 1.5 GHz. By increasing the frequency to 2.0 GHz, we gain a 33\% increase in raw processing power.

\begin{lstlisting}[caption={/boot/config.txt Overclocking},language=bash]
arm_freq=2000
over_voltage=6
\end{lstlisting}

\subsection{GPIO Allocation}

\begin{table}[H]
\centering
\caption{Raspberry Pi GPIO Pinout}
\begin{tabular}{|l|l|l|}
\hline
\textbf{GPIO} & \textbf{Function} & \textbf{Notes} \\ \hline
14 & UART TX & To ESP32 RX (GPIO 16) \\ \hline
15 & UART RX & From ESP32 TX (GPIO 17) \\ \hline
12 & NeoPixel PWM & DMA via rpi\_ws281x \\ \hline
10 & SPI MOSI & Display data \\ \hline
11 & SPI SCLK & Display clock \\ \hline
8 & SPI CE0 & Display chip select \\ \hline
\end{tabular}
\end{table}

\section{ESP32 Pin Mapping}

Defined in \texttt{src/uart/esp-code.ino}:

\begin{lstlisting}[style=cppstyle,caption={ESP32 Pin Definitions}]
// Motor Pins (L298N)
#define IN1 25  // Left motor forward
#define IN2 26  // Left motor backward
#define IN3 27  // Right motor forward
#define IN4 14  // Right motor backward

// Ultrasonic Sensors
#define TRIG1_PIN 4   #define ECHO1_PIN 5   // Left
#define TRIG2_PIN 18  #define ECHO2_PIN 19  // Center
#define TRIG3_PIN 21  #define ECHO3_PIN 22  // Right

// Analog Sensors
#define MQ2_PIN 34   // Gas sensor (ADC1_CH6)
#define SERVO_PIN 23 // Optional servo

// UART to Pi
#define RXD2 16
#define TXD2 17
\end{lstlisting}

\chapter{Power Distribution}

\section{Dual-Rail Architecture}

To prevent motor noise from corrupting logic signals, two isolated power supplies are used:

\begin{enumerate}
    \item \textbf{Logic Rail (5V, 3A)}: Official Raspberry Pi USB-C supply
    \item \textbf{Drive Rail (12V, Li-Ion)}: Powers L298N, ESP32, Motors
\end{enumerate}

\begin{figure}[H]
\centering
\begin{verbatim}
+----------------+     +-------------------------+
| Logic Rail (5V)|     | Drive Rail (12V Li-Ion) |
| USB-C 3A       |     | 2S/3S Battery Pack      |
+-------+--------+     +-----------+-------------+
        |                          |
   +----+----+              +------+------+
   | Pi 4    |              | L298N       |
   | Camera  |              | ESP32       |
   | USB Mic |              | Motors      |
   | Display |              | Sensors     |
   +---------+              +-------------+
        |                          |
        +------ COMMON GND --------+
\end{verbatim}
\caption{Power Distribution Schematic}
\end{figure}

\textbf{Critical}: Both rails must share a common ground for UART to function correctly.

% ============================================================================
% PART III: SOFTWARE DEEP DIVE
% ============================================================================
\part{Software Deep Dive}

\chapter{The Orchestrator}

\section{Overview}

\textbf{File}: \texttt{src/core/orchestrator.py} (368 lines)

The Orchestrator is the central coordination hub implementing a Phase-Driven Finite State Machine. It:

\begin{enumerate}
    \item \textbf{Binds} to both ZMQ buses (upstream and downstream)
    \item \textbf{Polls} for events with 100ms timeout
    \item \textbf{Dispatches} events to handler methods
    \item \textbf{Checks} timeouts for soft-deadline enforcement
    \item \textbf{Publishes} LED states and navigation commands
\end{enumerate}

\section{Phase Enum}

\begin{lstlisting}[caption={Phase Enumeration}]
class Phase(Enum):
    IDLE = auto()       # Waiting for wakeword
    LISTENING = auto()  # Capturing speech for STT
    THINKING = auto()   # Waiting for LLM response
    SPEAKING = auto()   # Playing TTS audio
    ERROR = auto()      # System error state
\end{lstlisting}

\section{Transition Table}

The \texttt{TRANSITIONS} class variable defines exactly which state changes are legal:

\begin{lstlisting}[caption={State Transition Table (17 Transitions)}]
TRANSITIONS = {
    (Phase.IDLE, "wakeword"): Phase.LISTENING,
    (Phase.IDLE, "auto_trigger"): Phase.LISTENING,
    (Phase.IDLE, "manual_trigger"): Phase.LISTENING,
    (Phase.LISTENING, "stt_valid"): Phase.THINKING,
    (Phase.LISTENING, "stt_invalid"): Phase.IDLE,
    (Phase.LISTENING, "stt_timeout"): Phase.IDLE,
    (Phase.THINKING, "llm_with_speech"): Phase.SPEAKING,
    (Phase.THINKING, "llm_no_speech"): Phase.IDLE,
    (Phase.SPEAKING, "tts_done"): Phase.IDLE,
    (Phase.IDLE, "health_error"): Phase.ERROR,
    (Phase.LISTENING, "health_error"): Phase.ERROR,
    (Phase.THINKING, "health_error"): Phase.ERROR,
    (Phase.SPEAKING, "health_error"): Phase.ERROR,
    (Phase.ERROR, "health_ok"): Phase.IDLE,
    (Phase.ERROR, "error_timeout"): Phase.IDLE,
}
\end{lstlisting}

\textbf{Design Insight}: This explicit table makes \textbf{illegal states impossible}. Any event not in this table is silently ignored.

\section{Main Event Loop}

\begin{lstlisting}[caption={Main Event Loop}]
def run(self) -> None:
    poller = zmq.Poller()
    poller.register(self.events_sub, zmq.POLLIN)

    while True:
        socks = dict(poller.poll(timeout=100))
        if self.events_sub in socks:
            topic, data = self.events_sub.recv_multipart()
            payload = json.loads(data)
            
            if topic == TOPIC_WW_DETECTED:
                self.on_wakeword(payload)
            elif topic == TOPIC_STT:
                self.on_stt(payload)
            elif topic == TOPIC_LLM_RESP:
                self.on_llm(payload)
            # ... other handlers
        
        self._check_timeouts()
        self._check_auto_trigger()
\end{lstlisting}

The 100ms poll timeout ensures timeouts are checked even if no messages arrive.

\chapter{Inter-Process Communication}

\section{Topic Definitions}

\textbf{File}: \texttt{src/core/ipc.py} (100 lines)

All 14 topics are defined as byte literals:

\begin{lstlisting}[caption={IPC Topic Constants}]
# Events (upstream: sensors -> orchestrator)
TOPIC_WW_DETECTED = b"ww.detected"
TOPIC_STT = b"stt.transcription"
TOPIC_LLM_RESP = b"llm.response"
TOPIC_VISN = b"visn.object"
TOPIC_ESP = b"esp32.raw"
TOPIC_HEALTH = b"system.health"

# Commands (downstream: orchestrator -> actuators)
TOPIC_LLM_REQ = b"llm.request"
TOPIC_TTS = b"tts.speak"
TOPIC_NAV = b"nav.command"
TOPIC_CMD_PAUSE_VISION = b"cmd.pause.vision"
TOPIC_CMD_LISTEN_START = b"cmd.listen.start"
TOPIC_CMD_LISTEN_STOP = b"cmd.listen.stop"
TOPIC_DISPLAY_STATE = b"display.state"
\end{lstlisting}

\section{Socket Factory Functions}

\begin{lstlisting}[caption={Pub/Sub Socket Factories}]
def make_publisher(config, *, channel="upstream", bind=False):
    upstream, downstream = _ipc_addrs(config)
    addr = upstream if channel == "upstream" else downstream
    sock = ctx.socket(zmq.PUB)
    (sock.bind if bind else sock.connect)(addr)
    return sock

def make_subscriber(config, *, topic=b"", channel="upstream"):
    # ... similar pattern
    sock.setsockopt(zmq.SUBSCRIBE, topic)
    return sock

def publish_json(sock, topic, payload):
    sock.send_multipart([topic, json.dumps(payload).encode()])
\end{lstlisting}

\chapter{Voice Pipeline}

\section{Overview}

\textbf{File}: \texttt{src/audio/unified\_voice\_pipeline.py} (602 lines)

The Unified Voice Pipeline consolidates:
\begin{enumerate}
    \item \textbf{Wakeword Detection} (Porcupine)
    \item \textbf{Speech-to-Text} (Faster-Whisper)
    \item \textbf{ZMQ Publishing} (event emission)
\end{enumerate}

\section{Pipeline State Machine}

\begin{lstlisting}[caption={Pipeline States}]
class PipelineState(Enum):
    IDLE = auto()         # Waiting for wakeword
    CAPTURING = auto()    # Recording user speech
    TRANSCRIBING = auto() # Running STT inference
    COOLDOWN = auto()     # Brief pause after TTS
\end{lstlisting}

\section{Wakeword Processing}

\begin{lstlisting}[caption={Porcupine Wakeword Detection}]
def _process_wakeword(self) -> None:
    frame_length = self._porcupine.frame_length  # 512 samples
    samples = self.audio.read_chunk(
        self._wakeword_consumer_id,
        num_samples=frame_length,
        timeout_ms=100
    )
    result = self._porcupine.process(samples.tolist())
    if result >= 0:  # Wakeword detected
        self._on_wakeword_detected()
\end{lstlisting}

\section{Silence Detection (VAD)}

Instead of a neural VAD, the pipeline uses RMS amplitude:

\begin{lstlisting}[caption={RMS-Based Voice Activity Detection}]
@staticmethod
def _calc_rms(samples: np.ndarray) -> float:
    if len(samples) == 0:
        return 0.0
    energy = np.mean(samples.astype(np.float32) ** 2)
    return min(1.0, math.sqrt(energy) / 32768.0)
\end{lstlisting}

\textbf{Why RMS?} A neural VAD can misclassify motor noise as speech. RMS combined with a 900ms silence window is more robust for a moving robot.

\section{Faster-Whisper Integration}

\begin{lstlisting}[caption={STT Transcription}]
def _transcribe(self, audio: np.ndarray):
    with tempfile.NamedTemporaryFile(suffix=".wav") as f:
        with wave.open(f.name, "wb") as wf:
            wf.setnchannels(1)
            wf.setsampwidth(2)
            wf.setframerate(16000)
            wf.writeframes(audio.astype("<i2").tobytes())
        
        segments, info = self._stt_model.transcribe(
            f.name,
            language="en",
            beam_size=1,     # Greedy for speed
            vad_filter=True,
        )
        text = " ".join(seg.text.strip() for seg in segments)
        return text, confidence
\end{lstlisting}

\textbf{Performance}: ~800ms for a simple command on Pi 4 @ 2.0 GHz.

\chapter{Vision Pipeline}

\section{Overview}

\textbf{File}: \texttt{src/vision/vision\_runner.py} (327 lines)

The vision pipeline performs real-time object detection with stale-frame protection.

\section{The Latest-Frame Problem}

OpenCV's \texttt{VideoCapture.read()} returns frames from an internal buffer. If inference takes 200ms, you process frames that are seconds old.

\section{Solution: Threaded Frame Grabber}

\begin{lstlisting}[caption={LatestFrameGrabber Thread}]
class LatestFrameGrabber(threading.Thread):
    def __init__(self, camera_index, target_fps=15.0):
        super().__init__(daemon=True)
        self.cap = cv2.VideoCapture(camera_index)
        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        self._lock = threading.Lock()
        self._latest_frame = None
    
    def run(self):
        while not self._stop_event.is_set():
            ret, frame = self.cap.read()
            if ret:
                with self._lock:
                    self._latest_frame = frame  # Overwrite
    
    def get_frame(self):
        with self._lock:
            return self._latest_frame.copy()
\end{lstlisting}

\section{Stale Frame Protection}

\begin{lstlisting}[caption={Stale Frame Filtering}]
frame, frame_time = grabber.get_frame()
frame_age = now - frame_time
if frame_age > 0.5:  # Skip frames > 500ms old
    logger.debug("Skipping stale frame")
    continue
\end{lstlisting}

\chapter{LLM Integration}

\section{Overview}

\textbf{File}: \texttt{src/llm/gemini\_runner.py} (306 lines)

The LLM runner provides cloud integration with conversation memory.

\section{Conversation Memory}

Unlike local LLMs, cloud APIs are stateless. The \texttt{ConversationMemory} class maintains context:

\begin{lstlisting}[caption={Memory Initialization}]
max_turns = int(llm_cfg.get("memory_max_turns", 10))
timeout_s = float(llm_cfg.get("conversation_timeout_s", 120.0))
self.memory = ConversationMemory(
    max_turns=max_turns,
    conversation_timeout_s=timeout_s,
)
\end{lstlisting}

This enables multi-turn conversations: ``Find the bottle'' → ``Now follow it''

\section{JSON Response Enforcement}

\begin{lstlisting}[caption={Strict JSON Output}]
generation_config["response_mime_type"] = "application/json"
\end{lstlisting}

This forces Gemini to return only valid JSON.

\chapter{Motor Control}

\section{ESP32 Firmware}

\textbf{File}: \texttt{src/uart/esp-code.ino} (491 lines)

\subsection{Main Loop (50ms Cycle)}

\begin{lstlisting}[style=cppstyle,caption={ESP32 Main Loop}]
void loop() {
    // 1. READ SENSORS
    long dist1 = readDistance(TRIG1_PIN, ECHO1_PIN);
    long dist2 = readDistance(TRIG2_PIN, ECHO2_PIN);
    long dist3 = readDistance(TRIG3_PIN, ECHO3_PIN);
    
    // 2. COLLISION AVOIDANCE (Highest Priority)
    checkCollision();
    
    // 3. TELEMETRY TO PI
    PiSerial.print("DATA:S1:"); PiSerial.print(dist1);
    // ... other values
    
    // 4. COMMAND INGESTION
    while (PiSerial.available() > 0) {
        // parse commands
    }
    
    delay(50); // 20Hz update rate
}
\end{lstlisting}

\subsection{Collision Avoidance}

\begin{lstlisting}[style=cppstyle,caption={Hardware Collision Avoidance}]
void checkCollision() {
    long minDist = 9999;
    for (int i = 0; i < 3; i++) {
        if (lastSensorDistances[i] > 0 && 
            lastSensorDistances[i] < minDist) {
            minDist = lastSensorDistances[i];
        }
    }
    
    if (minDist <= STOP_DISTANCE_CM) {       // <10cm
        emergencyStop();
        motorsEnabled = false;
    } else if (minDist <= WARNING_DISTANCE_CM) { // 10-20cm
        inWarningZone = true;
    } else {
        motorsEnabled = true;
    }
}
\end{lstlisting}

\subsection{Command Handler}

\begin{lstlisting}[style=cppstyle,caption={Command Processing}]
void handleCommand(String command, String source) {
    if (cmdu.startsWith("FORWARD")) {
        if (!motorsEnabled || obstacleDetected) {
            sendAck("FORWARD", "BLOCKED:OBSTACLE");
            return;
        }
        moveForward(255);
        sendAck("FORWARD", "OK");
    }
    // ... other commands
}
\end{lstlisting}

\section{Pi-Side UART Bridge}

\textbf{File}: \texttt{src/uart/motor\_bridge.py} (491 lines)

\begin{lstlisting}[caption={Pi-Side Safety Layer}]
def _check_pi_side_safety(self, cmd):
    direction = (cmd.direction or "").lower()
    
    # Always allow: stop, backward, turns
    if direction in ("stop", "backward", "left", "right"):
        return True, ""
    
    # For forward, check sensor data
    if direction == "forward" and self._last_sensor_data:
        sd = self._last_sensor_data
        if sd.obstacle:
            return False, "ESP32 obstacle detected"
        if sd.min_distance < self.STOP_DISTANCE_CM:
            return False, "Distance below threshold"
    return True, ""
\end{lstlisting}

% ============================================================================
% PART IV: INTEGRATION & OPERATIONS
% ============================================================================
\part{Integration \& Operations}

\chapter{Human Interface}

\section{LED Ring Service}

\textbf{File}: \texttt{src/piled/led\_ring\_service.py} (363 lines)

\begin{table}[H]
\centering
\caption{LED Color Scheme}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{State} & \textbf{Color} & \textbf{Pattern} & \textbf{Meaning} \\ \hline
idle & Dim Cyan & Breathing & Waiting for wakeword \\ \hline
wakeword\_detected & Bright Green & Flash & Acknowledged \\ \hline
listening & Bright Blue & Sweep & Capturing audio \\ \hline
transcribing & Purple & Pulse & STT processing \\ \hline
thinking & Pink & Pulse & LLM processing \\ \hline
speaking & Dark Green & Chase & Playing audio \\ \hline
error & Red & Blink & System error \\ \hline
\end{tabular}
\end{table}

\section{TFT Display Service}

\textbf{File}: \texttt{src/ui/display\_runner.py} (559 lines)

Uses pygame on framebuffer (\texttt{/dev/fb1}) for direct rendering without X11.

\chapter{End-to-End Execution}

\section{Latency Trace: ``Move Forward''}

\begin{table}[H]
\centering
\caption{End-to-End Latency Breakdown}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Timestamp} & \textbf{Event} & \textbf{Component} \\ \hline
T+0.000s & User says ``Hey Veera'' & Microphone \\ \hline
T+0.050s & Wakeword detected & Porcupine \\ \hline
T+0.051s & IDLE → LISTENING & Orchestrator \\ \hline
T+0.100s & LED turns green & LED Ring \\ \hline
T+0.950s & User says ``Move forward'' & Microphone \\ \hline
T+1.750s & STT returns text & Faster-Whisper \\ \hline
T+1.760s & LISTENING → THINKING & Orchestrator \\ \hline
T+2.800s & LLM returns response & Gemini API \\ \hline
T+2.810s & nav.command published & Orchestrator \\ \hline
T+2.820s & Wheels turn & ESP32/Motors \\ \hline
\end{tabular}
\end{table}

\textbf{Total Latency}: ~2.8 seconds\\
\textbf{Bottleneck}: Cloud API round-trip (1.0s)

\chapter{Systemd Services}

\section{Service Architecture}

\begin{table}[H]
\centering
\caption{Systemd Service Definitions}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Service} & \textbf{Module} & \textbf{Virtual Env} \\ \hline
orchestrator & src.core.orchestrator & stte \\ \hline
voice-pipeline & src.audio.unified\_voice\_pipeline & stte \\ \hline
llm & src.llm.gemini\_runner & llme \\ \hline
tts & src.tts.piper\_runner & ttse \\ \hline
vision & src.vision.vision\_runner & visn \\ \hline
uart & src.uart.motor\_bridge & stte \\ \hline
display & src.ui.display\_runner & dise \\ \hline
led-ring & src.piled.led\_ring\_service & stte \\ \hline
\end{tabular}
\end{table}

\section{Example Service File}

\begin{lstlisting}[caption={orchestrator.service},language=bash]
[Unit]
Description=Smart Car Orchestrator
After=network.target

[Service]
Type=simple
User=dev
WorkingDirectory=/home/dev/smart_car
Environment=PROJECT_ROOT=/home/dev/smart_car
EnvironmentFile=/home/dev/smart_car/.env
ExecStart=/home/dev/smart_car/.venvs/stte/bin/python \
    -m src.core.orchestrator
Restart=on-failure
RestartSec=3

[Install]
WantedBy=multi-user.target
\end{lstlisting}

\chapter{Testing \& Deployment}

\section{Test Suite}

\begin{lstlisting}[language=bash,caption={Running Tests}]
source .venvs/stte/bin/activate
pytest src/tests -v
# Expected: 10 passed in ~4s
\end{lstlisting}

\section{Deployment Steps}

\begin{enumerate}
    \item Clone repository
    \item Run \texttt{./setup\_envs.sh}
    \item Flash ESP32 firmware
    \item Copy systemd service files
    \item Enable and start services
\end{enumerate}

% ============================================================================
% APPENDICES
% ============================================================================
\appendix

\chapter{Configuration Reference}

\section{system.yaml Schema}

\begin{lstlisting}[caption={Complete Configuration},language=bash]
ipc:
  upstream: tcp://127.0.0.1:6010
  downstream: tcp://127.0.0.1:6011

audio:
  use_unified_pipeline: true
  hw_sample_rate: 48000
  mic_device: smartcar_capture

wakeword:
  engine: porcupine
  sensitivity: 0.75
  model: ${PROJECT_ROOT}/models/wakeword/hey-veera.ppn

stt:
  engine: faster_whisper
  sample_rate: 16000
  silence_threshold: 0.25
  engines:
    faster_whisper:
      model: tiny.en
      compute_type: int8

llm:
  engine: gemini
  gemini_model: gemini-2.5-flash
  temperature: 0.2
  memory_max_turns: 10

vision:
  model_path_onnx: ${PROJECT_ROOT}/models/vision/yolo11n.onnx
  input_size: [640, 640]
  target_fps: 15

nav:
  uart_device: /dev/serial0
  baud_rate: 115200
\end{lstlisting}

\chapter{Code Statistics}

\begin{table}[H]
\centering
\caption{Lines of Code by Module}
\begin{tabular}{|l|r|l|}
\hline
\textbf{Module} & \textbf{Lines} & \textbf{Key Insight} \\ \hline
orchestrator.py & 368 & 17-transition FSM \\ \hline
ipc.py & 100 & 14 ZMQ topics \\ \hline
unified\_voice\_pipeline.py & 602 & RMS-based VAD \\ \hline
vision\_runner.py & 327 & LatestFrameGrabber \\ \hline
motor\_bridge.py & 491 & Pi-side safety \\ \hline
gemini\_runner.py & 306 & ConversationMemory \\ \hline
esp-code.ino & 491 & Hardware interlocks \\ \hline
display\_runner.py & 559 & Framebuffer rendering \\ \hline
led\_ring\_service.py & 363 & Phase-driven LEDs \\ \hline
\textbf{Total} & \textbf{~5,200} & \\ \hline
\end{tabular}
\end{table}

% ============================================================================
% BIBLIOGRAPHY
% ============================================================================
\chapter*{References}
\addcontentsline{toc}{chapter}{References}

\begin{enumerate}
    \item \textbf{ZeroMQ Guide}. iMatix Corporation. \url{https://zguide.zeromq.org/}
    \item \textbf{Faster-Whisper}. SYSTRAN. \url{https://github.com/SYSTRAN/faster-whisper}
    \item \textbf{Porcupine Wake Word Engine}. Picovoice Inc. \url{https://picovoice.ai/products/porcupine/}
    \item \textbf{YOLOv11}. Ultralytics. \url{https://docs.ultralytics.com/}
    \item \textbf{Google Gemini API}. Google DeepMind. \url{https://ai.google.dev/}
    \item \textbf{Raspberry Pi GPIO Documentation}. Raspberry Pi Foundation. \url{https://www.raspberrypi.com/documentation/}
    \item \textbf{ESP32 Arduino Core}. Espressif Systems. \url{https://docs.espressif.com/}
    \item \textbf{Adafruit NeoPixel Library}. Adafruit Industries. \url{https://learn.adafruit.com/}
    \item \textbf{L298N Motor Driver Datasheet}. STMicroelectronics.
    \item \textbf{HC-SR04 Ultrasonic Sensor Datasheet}. Various manufacturers.
\end{enumerate}

\end{document}
